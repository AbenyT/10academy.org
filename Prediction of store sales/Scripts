# Required Libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor  # For optional XGBoost model
from sklearn.metrics import mean_squared_error, mean_absolute_error
import joblib
from datetime import datetime
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Load the dataset
# Assuming train_df is your pandas dataframe with 'Sales', 'Customers', and 'Date' columns
# Example: train_df = pd.read_csv('store_sales.csv')

# Preprocessing Step (2.1)
# Convert 'Date' column to datetime
train_df['Date'] = pd.to_datetime(train_df['Date'])

# Extract features from 'Date' column
train_df['DayOfWeek'] = train_df['Date'].dt.dayofweek  # Monday=0, Sunday=6
train_df['IsWeekend'] = train_df['DayOfWeek'] >= 5  # Flag for weekends
train_df['DayOfMonth'] = train_df['Date'].dt.day
train_df['Month'] = train_df['Date'].dt.month
train_df['Year'] = train_df['Date'].dt.year
train_df['IsStartOfMonth'] = train_df['DayOfMonth'] <= 10
train_df['IsMidMonth'] = (train_df['DayOfMonth'] > 10) & (train_df['DayOfMonth'] <= 20)
train_df['IsEndOfMonth'] = train_df['DayOfMonth'] > 20

# Generate custom feature (e.g., Days to Holiday)
holidays = pd.to_datetime(['2015-12-25', '2016-01-01'])  # Example holiday dates
train_df['DaysToHoliday'] = train_df['Date'].apply(lambda x: (holidays - x).days.min())

# Handle missing values - filling NaNs with median
train_df.fillna(train_df.median(), inplace=True)

# Feature and Target Separation
X = train_df.drop(columns=['Sales', 'Customers', 'Date'])
y = train_df['Sales']

# Split the data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Machine Learning Model with sklearn Pipelines (2.2)
# RandomForest Pipeline
pipeline_rf = Pipeline([
    ('scaler', StandardScaler()),  # Scaling
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))  # Random Forest Model
])

# GradientBoostingRegressor Pipeline
pipeline_gbr = Pipeline([
    ('scaler', StandardScaler()),  
    ('regressor', GradientBoostingRegressor(n_estimators=100, random_state=42))  
])

# XGBoost Pipeline (Optional)
pipeline_xgb = Pipeline([
    ('scaler', StandardScaler()),  
    ('regressor', XGBRegressor(n_estimators=100, random_state=42))  
])

# Train the models
pipeline_rf.fit(X_train, y_train)
pipeline_gbr.fit(X_train, y_train)
pipeline_xgb.fit(X_train, y_train)

# Make predictions
y_pred_rf = pipeline_rf.predict(X_val)
y_pred_gbr = pipeline_gbr.predict(X_val)
y_pred_xgb = pipeline_xgb.predict(X_val)

# Calculate RMSE for each model
rmse_rf = mean_squared_error(y_val, y_pred_rf) ** 0.5
rmse_gbr = mean_squared_error(y_val, y_pred_gbr) ** 0.5
rmse_xgb = mean_squared_error(y_val, y_pred_xgb) ** 0.5

print(f"RMSE (RandomForest): {rmse_rf}")
print(f"RMSE (GradientBoosting): {rmse_gbr}")
print(f"RMSE (XGBoost): {rmse_xgb}")

# Choose a Loss Function (2.3)
# MAE and MSE Evaluation
mae = mean_absolute_error(y_val, y_pred_rf)  # Example with RandomForest
mse = mean_squared_error(y_val, y_pred_rf)

print(f"Mean Absolute Error (RandomForest): {mae}")
print(f"Mean Squared Error (RandomForest): {mse}")

# Post Prediction Analysis (2.4)
# Feature Importance (using GradientBoostingRegressor)
feature_importances = pipeline_gbr.named_steps['regressor'].feature_importances_
features = X_train.columns
indices = np.argsort(feature_importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importance (GradientBoosting)")
plt.bar(range(X_train.shape[1]), feature_importances[indices], align='center')
plt.xticks(range(X_train.shape[1]), [features[i] for i in indices], rotation=90)
plt.show()

# Serialize Models (2.5)
# Save models with timestamp
timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
joblib.dump(pipeline_rf, f"sales_model_rf_{timestamp}.pkl")
joblib.dump(pipeline_gbr, f"sales_model_gbr_{timestamp}.pkl")
joblib.dump(pipeline_xgb, f"sales_model_xgb_{timestamp}.pkl")

# Building Model with Deep Learning (2.6)
# LSTM for Time Series Sales Prediction

# Scale data in (-1, 1) range for LSTM
scaler = MinMaxScaler(feature_range=(-1, 1))
X_scaled = scaler.fit_transform(X)
y_scaled = scaler.fit_transform(y.values.reshape(-1, 1))

# Reshape data for LSTM (samples, timesteps, features)
X_train_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))

# Define LSTM Model
model = Sequential([
    LSTM(50, activation='relu', input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),
    Dense(1)
])

# Compile the LSTM Model
model.compile(optimizer='adam', loss='mse')

# Train the LSTM Model
model.fit(X_train_lstm, y_scaled, epochs=20, batch_size=32)

# Predict with LSTM
y_pred_lstm = model.predict(X_train_lstm)

# Inverse transform the scaled predictions and targets to original scale
y_pred_lstm_orig = scaler.inverse_transform(y_pred_lstm)
y_train_lstm_orig = scaler.inverse_transform(y_scaled)

# Evaluate LSTM predictions
lstm_mse = mean_squared_error(y_train_lstm_orig, y_pred_lstm_orig)
lstm_rmse = np.sqrt(lstm_mse)

print(f"RMSE (LSTM): {lstm_rmse}")
